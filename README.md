# ITtensive
Tasks from python-advanced course
Это описание решения задач по курсу "Продвинутый python"
1.	Импорт данных. Задача состоит в получении фрейма данных по url из файла csv и вычислении среднего по столбцу.
Для решения импортируется библиотека pandas, методом .read_csv() формируется фрейм данных и для столбца ‘Calls’ вычисляется среднее методом .mean().
2.	Данные из нескольких источников. Задача состоит в получении значения в одной таблице по условию из второй.
Импортируем pandas, методом .read_csv() формируем два фрейма данных. Переименовываем столбцы, чтобы установить единый индекс. Объединяем таблицы методом .merge() и по заданному условию находим искомое значение.
3.	Выделение данных. Задан набор статистических числовых данных, изменяющихся по датам. Задача состоит в определении даты, с которой отношение двух величин перешагнуло какое-то пороговое значение.
Определяется функция, принимающая как аргумент структуру данных и возвращающая её срез, соответствующий условию. В нашем случае это ограничение отношения значений 6-го, (UnemployedDisabled),  и 7-го, (UnemployedTotal), столбцов величиной менее 2%.
Применяем эту функцию к нашей структуре методом apply() по отношению к столбцам (axis=1). Первая строка полученного среза соответствует дате, с которой начинает выполняться заданное условие.
4.	Предсказание на 2020 год. Для фрейма данных из предыдущего задания необходимо построить модель линейной регрессии процента безработных инвалидов и дать предсказание на 2020 год по этой величине.
С применением lambda-функции добавляем столбец со значением процента безработных инвалидов. Группируем структуру данных по годам и с помощью метода filter() удаляем из структуры годы, где количество строк менее шести. Создаем структуру данных для средних значений по году (groupby('Year').mean()). Создаем массивы  x (год) и y (процент безработных инвалидов) и приводим в соответствие их размеры (метод reshape()).
Строим модель линейной регрессии (model.fit(x, y)) и представляем х и у в виде графика разброса plt.scatter(). Добавляем по оси х 2020 год и соответственно увеличиваем на 1 размер массива х. По созданной модели предсказываем значение 2020 года методом predict() и представляем эту модель в виде графика (plt.plot()).
5.	Получение координат по API. Используя запрос по API, надо найти долготу города Самары.
Импортируем библиотеки requests и json. На сайте API Геокодера Яндекса получаем ключ и отправляем запрос в соответствии с документацией на сайте, формат ответа указываем json. Из ответа выводим значение долготы (по умолчанию указывается первой координатой).
6.	Получение котировок. На сайте МосБиржи надо найти тикер с максимальным ростом на заданную дату.
Для решения задачи импортируем библиотеки requests, pandas и BeautifulSoup. Запрашиваем интересующую нас страницу методом get(), результат запроса r выводим в виде html, используя BeautifulSoup(r.content). В поисковую строку вводим «тикер» и находим с его помощью таблицу котировок. В коде находим класс таблицы и по нему «вычленяем» ее из html: table = html.find('table', {'id':'marketDataList'}). Преобразуем таблицу в читаемый вид. Для этого создаем пустой список rows, создаем список строк таблицы trs = html.find_all(‘tr’). Циклом преобразуем каждую строку полученной таблицы trs, при этом в каждой строке формируем список элементов, «очищенных» методом get_text(), и добавляем каждую строку к таблице rows. Создаем фрейм данных data из нашего массива rows и соответствующих названий столбцов. Приводим интересующий нас столбец к типу float, назначаем его индексом и сортируем по нему наш фрейм (по убыванию). Выводим первое значение столбца «Тикер».
7.	Парсинг интернет-магазина. По странице интернет-магазина надо найти разницу в характеристике двух холодильников.
Импортируем библиотеки request и BeatifulSoup. Указываем агента. Представляем полученный по запросу контент в виде BeatifulSoup. Из полученного контента методом find_all() находим по классу ссылки (a), ведущие на страницу марки холодильника. Назначаем две переменные, соответствующие интересующим нас маркам холодильников, и циклом с условием находим соответствующие им ссылки. Определяем функцию для нахождения нужной нам характеристики (объема) по ссылке. Функция делает запрос на страницу интернет-магазина с этой ссылкой, преобразовывает ответ в BeatifulSoup, находит в нем фрагмент с характеристиками по классу (класс находим поисковиком по слову «объем»). Функция возвращает числовое значение объема холодильника (идет третьим в характеристиках, поэтому индекс [2]). При условии, если ссылки на интересующие нас марки холодильников успешно найдены, выводим их абсолютную разницу.
8.	Загрузка результатов в базу данных. Необходимо собрать данные о холодильниках на странице интернет-магазина и в SQLite базе данных загрузить в таблицу.
Для создания базы данных загружаем SQLite с сайта https://www.sqlite.org/download.html (Распаковываем два архива sqlite-dll и sqlite-tools в одну папку. Создаем базу данных: из командной строки заходим в SQLite (cd SQLite), запускаем sqlite3.exe, через пробел указывая базу данных data.db3. при этом создается база данных db3, выходим из нее (ctrl +z), закрываем консоль.
Импортируем библиотеки sqlite3, request, BeatifulSoup. Определяем функцию find_number(text) для получения числовых значений из текста. Опредляем функцию find_data(), которая принимает как аргумент ссылку, отправляет запрос, полученный контент преобразует в формат BeautifulSoup и из него извлекает данные для таблицы. Тег для наименования холодильника находим по слову «Саратов» это h1 class= “_3TfWusA7bt  _26mXJDBxtH”, получаем из него текст методом get_text(). Тег для цены находим по значению цены, это span data-tid="c3eaad93", также получаем текст, а из текста при помощи функции find_number() получаем числовое значение цены с типом integer. Теги для характеристик находим также, как в предыдущем задании, присваиваем всем характеристикам нулевые значения и циклом добавляем значения в таблицу. Габариты находим по выражению ШхВхГ, делим их двоеточием, берем второй член, его в свою очередь делим через «х», полученное число приводим к типу float. От «глубины» отсекаем единицы измерения, для этого делим ее через пробел и берем первый член. К тегам, содержащим «Общий объем» и «Объем холодильной камеры» для получения числового значения применяем функцию find_number(). Функция find_data() возвращает url, наименование, цену, ширину, высоту, глубину, объем холодильника и объем морозильной камеры, если она есть (если ее нет, функция оставляет заданное первоначальное значение 0). Запрашиваем страницу интернет-магазина методом requests.get(). Получаем контент в виде BeautifulSoup, находим класс ссылок (тег "a", class "_3ioN70chUh"), содержащих kholodilnik-saratov, формируем список этих ссылок links методом find_all(). И циклом for выводим из него массив, содержащий атрибут "href" и слово «Саратов» в тексте.
Создаем соединение с нашей базой данных sqlite3.connect(«путь»), создаем курсор db для отправки запросов и получения результатов. Создаем пустую таблицу в базе данных db.execute("""CREATE TABLE…"""), завершаем модификацию conn.commit(). Заполняем таблицу db.executemany("""INSERT INTO…""") данными из массива data, завершаем модификацию conn.commit().
Выводим полученную таблицу, выводим ответ на вопрос. Закрываем соединение с базой данных.
9.	Выбор типа визуализации. Надо выбрать типы диаграмм и построить их для двух серий данных из таблицы результатов по ЕГЭ в Москве за два года.
Импортируем библиотеки pandas и matplotlib.pyplot. Формируем таблицу данных по заданной ссылке методом read_csv(). Названия районов и административных округов сокращаем до одного слова и приводим к типу "category". Подписываем название «ЕГЭ в Москве» с указанием размера шрифта.  Назначаем индексом столбец ‘Year’, выбираем срез 2018-2019 годы, сбрасываем индекс. Устанавливаем размер холста и расположение диаграмм. Создаем фрейм data_adm из data, назначив индексом "AdmArea". Группируем серию data_adm["PASSES_OVER_220"] по "AdmArea", суммируем по округам, сортируем по убыванию и представляем в виде круговой диаграммы (12 значений нагляднее будут выглядеть именно в круговой диаграмме).
Для второй диаграммы также устанавливаем положение на холсте, подписываем наименование. Из фрейма data_adm берем срез data_adm.loc["Северо-Западный"], переназначаем индекс "District". Из полученного фрейма берем серию data_district["PASSES_OVER_220"], группируем её по району, с вычислением сумм и также строим круговую диаграмму.
10.	Разбор данных с марафона. В массиве данных надо найти коррелирующие серии, построить для них график jointplot и найти коэффициент корреляции.
Импортируем библиотеки matplotlib.pyplot, seaborn, pandas, scipy.stats. Определяем функцию, выражающую временные величины в секундах. С помощью метода read_csv() получаем данные с марафона, представляем их в виде таблицы data. В таблице для каждого участника представлены возраст, пол, время прохождения половины дистанции и финальное время. Приводим к секундам все временные величины с применением функции convert_time(), принимающей время в формате ч:м:с и возвращающей время в секундах. Из данных полученной таблицы с помощью библиотеки seaborn методом pairplot() строим графики всех пар переменных, с помощью параметра hue цветом выделяем пол участников. Находим единственную коррелирующую пару – split-final и для нее строим диаграмму методом jointplot(). Коэффициент корреляции для этой пары вычисляем с помощью библиотеки scipy.stats методом pearsonr().
11.	Визуализация графиков акций. По заданию необходимо получить данные в формате csv со страницы, сопоставить поведение двух переменных в разные годы и найти общую характерную точку.
Для решения потребуются библиотеки datetime, pandas, matplotlib.pyplot, rcParams, импортируем их, получаем данные методом read_csv() и приводим дату к формату день.месяц.год. С помощью метода date_range() создаем DatetimeIndex dates от первой до последней даты в нашем фрейме, определяем индексом столбец ‘Date’, затем переназначаем индекс на dates  с заполнением недостающих значений из предыдущей строки. Добавляем столбец ‘Day’ с номером дня года и присваиваем наименование индексу ‘Date’. Создаем два фрейма: data_2019 (со значениями ‘Close’ 2019 года) и data_2017 (со значениями экспоненциального среднего от ‘Max’ 2017 года), обоим назначаем индекс ‘Day’.
Задаем размеры холста, указываем положение нашей области и строим в ней оба наших графика. Последнее пересечение, после которого Close_2019 превысило Max_ewm_20-2017 – это наша искомая точка. Для определения даты создаем фрейм data_fall с условием, что значение в серии data_2019['Close'] меньше соответствующего значения в data_2017, сортируем по убыванию и выводим первое значение даты.
12.	Объекты культурного наследия в России. Используя геоданные надо построить фоновую картограмму по количеству объектов в регионах России.
Импортируем необходимые библиотеки matplotlib.pyplot,  geopandas, pandas, descartes. Скачиваем по ссылке архив с набором данных по объектам культурного наследия. Считываем его (read_csv()), оставляем только интересующие на столбцы ‘Регион’ и ‘Объект’, переводим шрифт названия регионов в регистр uppercase (то же сделаем и для второй таблицы, чтобы названия были отражены в одинаковом формате). Сгруппировав полученную таблицу по регионам с подсчетом количества объектов, получаем таблицу mem с количество объектов культурного наследия для каждого региона. Считываем таблицу геоданных и также переводим шрифт региона (здесь регион в столбце ‘NL_NAME_1’) в uppercase. Лучше работать с локальным файлом, на с сайта файл не загрузился, поэтому читаем его по ссылке. Объединяем обе таблицы в таблицу data с общим столбцом ‘NL_NAME_1’ на основе таблицы геоданных data. Выводим строки с неопределенными значениями по Объектам для определения несовпадающих названий регионов print(data[data["Объект"].isnull()]) и  приводим их к общему названию, переименовав их в одной из исходных таблиц в соответствие с другой. Переводим вид карты из сферической к Меркатору. Создаем и выводим картограмму. Для наглядности ограничиваем лимит по оси х от 2е6 до 2е7, отрезая при этом Чукотку. Перебираем строки нашей таблицы и выводим как аннотацию количество объектов с приведением к типу integer, располагаем аннотацию в геометрическом центре каждого региона.
13.	Cборка PDF документа. Используя данные с сайта, необходимо построить круговую диаграмму посещаемости библиотек с указанием района с самой высокой посещаемостью и представить все в виде файла pdf.
Для решения импортируем все необходимые библиотеки, отправляем запрос, полученные данные преобразуем в DataFrame. Из столбца "ObjectAddress", представляющего собой свою структуру данных «вытаскиваем» интересующий нас 'District', создаем из него столбец в нашу таблицу, а "ObjectAddress" удаляем. Для получения нужной нам структуры data_district группируем таблицу data по району с суммированием по посещаемости, сортировкой по убыванию, берем первые двадцать строк полученной таблицы. Создаем холст и область нашей диаграммы, создаем круговую диаграмму и сохраняем ее как файл png.
Устанавливаем параметры pdf-файла, задаем формат, размещаем текст и изображение, сохраняем файл.
Создаем список файлов для добавления в отчет.  Используя методы PdfFileMerger() и PdfFileReader(), с помощью цикла for добавляем файлы из списка в общий файл и записываем его методом write().
14.	Геральдические символы Москвы. По условию задания необходимо сгенерировать PDF документ из списка флагов и гербов районов Москвы на основе данных, полученных по предоставленным ссылкам.
Для решения импортируем библиотеки pandas, pdfkit, Template. Считываем данные методом read_csv(). В VS-code создаем шаблон herald.html. Методом render() в шаблон herald.html предоставляем данные из таблицы data, перебирая ее построчно, и сохраняем полученные данные как html. В шаблоне herald.html циклом for выведены контент и изображение для всех районов Москвы с условием ввода на новую страницу для всех, кроме первого. С помощью библиотеки pdfkit и приложения wkhtmltopdf задаем конфигурацию pdf-документа (задаем при этом размер страницы и номер страницы как заголовок справа), создаем из файла html pdf-документ, используя метод from_string().
15.	Многостраничный отчет. Используя данные по активностям в парках Москвы, необходимо создать PDF документ с диаграммой и таблицей активностей в этих парках.
Для решения импортируем необходимые библиотеки, запрашиваем с сайта данные, представляем их в виде Фрейма данных, смотрим на первые строки таблицы, выбираем интересующие нас столбцы, заполняем неопределенные данные нулями и «вытаскиваем» название парка из словаря NameOfPark. Переименовываем столбцы на русский для представления в отчете, выводим количество активностей Тайцзицюань. Создаем фрейм data_count из десяти самых активных, переназначив индексом «Парк», с группировкой по «Парку» с подсчетом активностей.
Создаем холст и область нашей диаграммы, создаем саму диаграмму и сохраняем ее в виде объекта BytesIO(), его мы непосредственно преобразуем в формат base64. Для таблицы увеличиваем количество символов в ячейке до 1000 при помощи метода set_option(), чтобы поместилось расписание. В VS-code создаем шаблон park_activity.html, в шаблон методом render() передаем изображение в формате base64 и данные в виде html-таблицы, получив ее методом data.to_html(). Создаем конфигурацию и параметры для pdf-файла и создаем его на основе нашего файла html при помощи метода pdfkit().
16.	Автоматические отчеты. По условию задания необходимо создать отчет в формате html на основе файла csv, полученного по запросу, и передать его на электронный адрес.
Для создания отчета импортируем необходимые библиотеки, считываем файл по заданной ссылке, берем из него интересующий нас временной срез и ограничиваем данные необходимыми столбцами, неопределенные значения меняем на нулевые. Сокращаем название округа до одного значимого слова и приводим его к типу ‘category’. Переименовываем колонки для отчета, сортируем по убыванию, увеличиваем ограничение числа символов в ячейке (определенных по умолчанию может не хватить). Выводим общее количество отличников в Москве по заданному периоду. Создаем фрейм data_area для отчета: определяем Округ индексом, группируем по нему с суммированием количества отличников и сортировкой по убыванию. Создаем холст и область диаграммы, выносим последние, самые узкие сектора для читаемости цифровых значений на них. Создаем круговую диаграмму по нашим данным и сохраняем ее в виде объекта BytesIO(). Преобразуем изображение в формат base64 и создаем html файл на основе заранее подготовленного шаблона Distr_over_220_in_areas.html, изображения в формате base64 и первой строки нашей таблицы, приведенной к формату html. Задаем конфигурацию страницы и создаем pdf-файл.
Для отправки по электронной почте импортируем библиотеки smtplib, encoders, MIMEText, MIMEBase, MIMEMultipart. Задаем параметры нашего сообщения, подключаемся к серверу smpt, заводим логин и пароль отправителя, адрес получателя, отключаемся от сервера.
